{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+DI/X0R8tW2dz9EEGvujc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitojha2705/ankitojha2705-CRISP_DM_SEMMA_AND_KDD-Models/blob/main/KDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Selection\n",
        "- Objective: Identify and gather the data needed for analysis.\n",
        "- Action: Download and load the dataset using pandas."
      ],
      "metadata": {
        "id": "70lTanlhb77L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdz04wTnbBXw",
        "outputId": "abb974d6-b750-484b-f33f-5da79b7ab4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded Successfully\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "file_path = path + '/creditcard.csv'  # Adjust the file name if necessary\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Dataset Loaded Successfully\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "- Objective: Clean the data to improve its quality. This includes handling missing values, removing duplicates, and correcting data types.\n",
        "- Action: Check for missing values and duplicates, then address them"
      ],
      "metadata": {
        "id": "0gzkllMxcOkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values in Each Column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values (if any) - for this dataset, there are usually no missing values\n",
        "df = df.fillna(df.median())  # Fill missing values with the median\n",
        "\n",
        "# Check for duplicates and remove them\n",
        "print(\"Number of Duplicate Rows:\", df.duplicated().sum())\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Check data types and convert if necessary\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztiAPip5bHH6",
        "outputId": "e300fad8-45d5-4849-f364-c99d1fa540f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values in Each Column:\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "Number of Duplicate Rows: 1081\n",
            "\n",
            "Data Types:\n",
            "Time      float64\n",
            "V1        float64\n",
            "V2        float64\n",
            "V3        float64\n",
            "V4        float64\n",
            "V5        float64\n",
            "V6        float64\n",
            "V7        float64\n",
            "V8        float64\n",
            "V9        float64\n",
            "V10       float64\n",
            "V11       float64\n",
            "V12       float64\n",
            "V13       float64\n",
            "V14       float64\n",
            "V15       float64\n",
            "V16       float64\n",
            "V17       float64\n",
            "V18       float64\n",
            "V19       float64\n",
            "V20       float64\n",
            "V21       float64\n",
            "V22       float64\n",
            "V23       float64\n",
            "V24       float64\n",
            "V25       float64\n",
            "V26       float64\n",
            "V27       float64\n",
            "V28       float64\n",
            "Amount    float64\n",
            "Class       int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Data Transformation\n",
        "- Objective: Transform the data for better analysis. This can include encoding categorical variables and scaling numerical features.\n",
        "- Action: Since this dataset has only numerical features, weâ€™ll focus on scaling."
      ],
      "metadata": {
        "id": "dcxu2i2JcYvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the numerical features\n",
        "df_scaled = df.copy()\n",
        "df_scaled.iloc[:, :-1] = scaler.fit_transform(df.iloc[:, :-1])  # Exclude the 'Class' column from scaling\n",
        "\n",
        "print(\"\\nData After Scaling:\")\n",
        "print(df_scaled.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GFnCM90buz7",
        "outputId": "3e93612c-924b-42c0-94ee-aa2cd9219abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data After Scaling:\n",
            "       Time        V1        V2        V3        V4        V5        V6  \\\n",
            "0 -1.996823 -0.701082 -0.041687  1.680101  0.976623 -0.247020  0.348012   \n",
            "1 -1.996823  0.608792  0.164138  0.109279  0.318998  0.042258 -0.060980   \n",
            "2 -1.996802 -0.700336 -0.811337  1.174270  0.270648 -0.366756  1.352655   \n",
            "3 -1.996802 -0.499064 -0.109972  1.187383 -0.608355 -0.008814  0.937245   \n",
            "4 -1.996781 -0.597606  0.535539  1.025470  0.287092 -0.297036  0.072873   \n",
            "\n",
            "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
            "0  0.193700  0.084434  0.333534  ... -0.024777  0.383483 -0.177444  0.110157   \n",
            "1 -0.065656  0.072903 -0.231703  ... -0.311372 -0.881454  0.162081 -0.561503   \n",
            "2  0.643223  0.210788 -1.381169  ...  0.343094  1.065068  1.457772 -1.138484   \n",
            "3  0.192079  0.320843 -1.264664  ... -0.149093  0.007299 -0.305465 -1.941446   \n",
            "4  0.481517 -0.228725  0.747917  ... -0.012516  1.101780 -0.220709  0.232904   \n",
            "\n",
            "        V25       V26       V27       V28    Amount  Class  \n",
            "0  0.247059 -0.392622  0.333033 -0.065850  0.244200      0  \n",
            "1  0.321175  0.260854 -0.027154  0.043219 -0.342584      0  \n",
            "2 -0.628161 -0.288861 -0.144325 -0.183824  1.158900      0  \n",
            "3  1.242487 -0.460694  0.154039  0.185687  0.139886      0  \n",
            "4 -0.394800  1.041677  0.550001  0.654234 -0.073813      0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Data Mining"
      ],
      "metadata": {
        "id": "wc71mHlgckUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df_scaled.drop('Class', axis=1)  # Features\n",
        "y = df_scaled['Class']  # Target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nModels Trained Successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBb4wsm9cerU",
        "outputId": "2115ded6-4b34-438b-a861-2dd2bea6849a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Models Trained Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Interpretation/Evaluation"
      ],
      "metadata": {
        "id": "wGwouvn4cr5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "print(\"\\nLogistic Regression Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_log_reg))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_log_reg))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_log_reg))\n",
        "\n",
        "# Evaluate Random Forest\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcY8W1xicoiP",
        "outputId": "61e11b9e-2b67-4407-af24-a0a14085c551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Metrics:\n",
            "Accuracy: 0.999177612255927\n",
            "Precision: 0.8809523809523809\n",
            "Recall: 0.5522388059701493\n",
            "F1 Score: 0.6788990825688074\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.9995065673535563\n",
            "Precision: 0.9509803921568627\n",
            "Recall: 0.7238805970149254\n",
            "F1 Score: 0.8220338983050848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aZATnaHjBwO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}